{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import ast\n",
    "from llama import Llama\n",
    "import fire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load data using cuDF for GPU acceleration\n",
    "file_path = '../data/unlabelled-desc-name-topics.csv'\n",
    "\n",
    "# Using cuDF to read the CSV file into GPU memory\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search to understand the nature of the elusive dark matter in our universe, making up 85% of its mass, is amongst the highest scientific priorities around the world. Terrestrial experiments focus efforts on Weakly Interacting Massive Particles (WIMPs) with ultra-low background experiments, operating many tonnes of target mass in deep underground sites. Such experiments have swept the bulk of the available electroweak parameter space for WIMPs and in the next decade will approach an irreducible background from coherent scattering of neutrinos - indistinguishable from WIMPs. Internationally, efforts are ramping up to explore new avenues towards the first definitive detection of galactic dark matter: quantum technologies is an exciting new frontier that may provide the breakthrough.\n",
      "\n",
      "Levitating nanospheres, opto-mechanically held with high precision, represent targets with unprecedented sensitivity to dark matter scattering. Our experimental setup holds a 150 nm quartz sphere, represented in Figure 1, and has recently indicated recoil sensitivity to local alpha-particle emission. In this project, we will investigate through the experimental facility and Monte Carlo simulations the sensitivity to neutron and gamma-ray sources as well. We expect that our target can discriminate between them whilst also providing directional information and sensitivity to free electrons. Such capability would be revolutionary in the global hunt for dark matter: sensing the direction of incoming particles lowers the number of events required for discovery dramatically and can penetrate the irreducible background from coherent neutrino scattering. Moreover, the system would be sensitive to a vast range of well-motivated low-mass WIMP and non-WIMP thermal relic dark matter candidates, as well as beyond the Standard Model and exotic neutrino physics, all of which have remained inaccessible to-date.\n"
     ]
    }
   ],
   "source": [
    "print(df['description'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/prompts/system-prompt-llama-3.txt') as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "ckpt_dir = '/home/rz20505/Documents/ask-jgi/Meta-Llama-3-8B-Instruct'\n",
    "tokenizer_path = '/home/rz20505/Documents/ask-jgi/Meta-Llama-3-8B-Instruct/tokenizer.model'\n",
    "temperature = 0.6\n",
    "top_p = 0.9\n",
    "max_seq_len = 512\n",
    "max_batch_size = 6\n",
    "max_gen_len = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff for distributed pytorch setup\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Set environment variables manually for a single-node, single-process setup\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "\n",
    "# Initialize the process group\n",
    "torch.distributed.init_process_group(\"nccl\", rank=0, world_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sample of data for trial to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce labels for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 6.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mERROR: \u001b[0mCould not consume arg: --f=/home/rz20505/.local/share/jupyter/runtime/kernel-v2-32362427Tqp9mT3Xmtb.json\n",
      "Usage: ipykernel_launcher.py\n",
      "\n",
      "For detailed information on this command, run:\n",
      "  ipykernel_launcher.py --help\n"
     ]
    },
    {
     "ename": "FireExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mFireExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rz20505/miniforge3/envs/text-labels/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Get above working and then use gpt-generated code\n",
    "def extract_topics():\n",
    "    generator = Llama.build(\n",
    "        ckpt_dir=ckpt_dir,\n",
    "        tokenizer_path=tokenizer_path,\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_batch_size=max_batch_size,\n",
    "        # model_parallel_size=1\n",
    "    )\n",
    "\n",
    "fire.Fire(extract_topics)\n",
    "\n",
    "\n",
    "# So far these efforts have failed because of difficulties converting the torchrun functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-labels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
